{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doOY4afYt6Vc",
        "outputId": "2bcc7d18-5912-4328-b6e8-3e2010d4933c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting BPEmb\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BPEmb) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from BPEmb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from BPEmb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2024.6.2)\n",
            "Installing collected packages: BPEmb\n",
            "Successfully installed BPEmb-0.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install BPEmb\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from bpemb import BPEmb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers"
      ],
      "metadata": {
        "id": "oS-oqKutudQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many parts to Transformers, we wil start with Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "N4bw44cFuvyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$"
      ],
      "metadata": {
        "id": "NUPP9XDtvKNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside every attention head there is scaled dot product with uses query, key and value to calculate attention weights, we will implement that first"
      ],
      "metadata": {
        "id": "LB4YqJwwvlb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_score = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "\n",
        "  return tf.matmul(weights, value), weights\n"
      ],
      "metadata": {
        "id": "dyGV_UAguZ1l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose our queries, keys, and values are each a length of 3 with a dimension of 4.\n",
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys = np.random.rand(seq_len, embed_dim)\n",
        "values = np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\",queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfeXT_3TyCoG",
        "outputId": "5eb7f90d-395e-4589-b18a-2674227bca33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.11237853 0.48299194 0.62938949 0.6126515 ]\n",
            " [0.93497659 0.57699119 0.56262699 0.54204357]\n",
            " [0.19426278 0.80967871 0.43821061 0.78351492]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWSZFsJLytuc",
        "outputId": "5c53c3a0-3955-40de-e061-157c139631ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tf.Tensor(\n",
            "[[0.6199776  0.20539664 0.588922   0.22266714]\n",
            " [0.65892184 0.2145942  0.64545256 0.2213564 ]\n",
            " [0.63038975 0.20718662 0.6035171  0.2216724 ]], shape=(3, 4), dtype=float32) \n",
            "\n",
            "Weights\n",
            " tf.Tensor(\n",
            "[[0.27230927 0.30554375 0.422147  ]\n",
            " [0.30628258 0.24092954 0.45278782]\n",
            " [0.27987507 0.28848448 0.43164048]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1SLWkHQgy4nQPFvvjG5_V8UTtpSAJ2zrr)"
      ],
      "metadata": {
        "id": "wbqc4oSO3ETs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "print(f\"Dimension of each head: {head_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yin-hKARy8PY",
        "outputId": "33c18ab1-21c9-4027-f0c0-63c7d6b9cb87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of each head: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using separate weight matrices per head"
      ],
      "metadata": {
        "id": "aoBODCyb3r6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose these are our input embeddings\n",
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXLVq789zkx_",
        "outputId": "29fb8513-c71a-4c22-c3c9-5ad38912a99c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.7 0.3 0.5 0.1 1.  0.6 0.6 0.5 0.1 0.7 0.9 0.6]\n",
            "  [0.2 0.4 0.9 0.8 0.1 0.1 0.8 0.1 0.  0.9 0.4 0.3]\n",
            "  [0.8 0.4 0.1 0.  0.8 0.5 0.6 0.3 0.8 0.4 0.5 0.7]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember each weight matrix should have a dimension of $\\text{d}\\ \\text{x}\\ \\text{d/h}$."
      ],
      "metadata": {
        "id": "rBCYpzTP5tyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring query, key and value\n",
        "\n",
        "# query\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# key\n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# value\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)"
      ],
      "metadata": {
        "id": "nBLng_bh4Zr-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9doNJR_5942",
        "outputId": "14d56762-67d2-4470-8118-730daf12b821"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.8 0.3 0.9 0.2]\n",
            " [0.5 0.6 0.7 0.8]\n",
            " [0.5 0.2 0.2 0.8]\n",
            " [0.7 0.3 0.3 0.7]\n",
            " [0.8 0.9 0.7 0.6]\n",
            " [0.3 0.3 1.  0. ]\n",
            " [0.9 0.2 0.6 0.7]\n",
            " [0.1 0.  0.3 1. ]\n",
            " [1.  0.4 0.  0.1]\n",
            " [0.4 0.7 0.8 0. ]\n",
            " [0.1 0.8 0.3 0.9]\n",
            " [0.6 0.8 0.6 0.9]]\n",
            "wq1:\n",
            " [[0.4 0.1 0.5 0.5]\n",
            " [0.2 0.2 0.6 0.2]\n",
            " [0.8 0.9 0.7 0.4]\n",
            " [0.3 0.9 0.3 0.4]\n",
            " [0.6 0.6 0.4 0.5]\n",
            " [0.9 0.4 0.4 0.8]\n",
            " [0.7 0.2 0.9 0.1]\n",
            " [0.6 0.7 0.8 0.8]\n",
            " [0.3 0.6 0.9 0. ]\n",
            " [0.1 0.3 0.6 1. ]\n",
            " [0.9 0.7 0.9 0.2]\n",
            " [0.3 0.9 0.3 0.4]]\n",
            "wq2:\n",
            " [[0.4 0.1 0.5 0.5]\n",
            " [0.2 0.2 0.6 0.2]\n",
            " [0.8 0.9 0.7 0.4]\n",
            " [0.3 0.9 0.3 0.4]\n",
            " [0.6 0.6 0.4 0.5]\n",
            " [0.9 0.4 0.4 0.8]\n",
            " [0.7 0.2 0.9 0.1]\n",
            " [0.6 0.7 0.8 0.8]\n",
            " [0.3 0.6 0.9 0. ]\n",
            " [0.1 0.3 0.6 1. ]\n",
            " [0.9 0.7 0.9 0.2]\n",
            " [0.3 0.9 0.3 0.4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate query, key and value for first head\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Generate query, key and value for second head\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Generate query, key and value for third head\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)"
      ],
      "metadata": {
        "id": "Ux5uIOAd6BRq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8XQfUAT7Xys",
        "outputId": "4159b908-6ebe-4e12-e6f1-55449c5f6296"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[3.43 3.45 3.97 3.73]\n",
            "  [2.79 2.19 2.58 2.99]\n",
            "  [3.68 3.05 3.42 2.92]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[4.18 4.63 3.26 3.6 ]\n",
            "  [3.11 3.43 1.69 2.58]\n",
            "  [3.42 3.97 2.96 3.15]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[3.74 4.47 2.83 3.44]\n",
            "  [3.1  2.87 1.45 2.35]\n",
            "  [3.37 3.97 2.53 3.27]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we pass query, key and value to self-attention operation"
      ],
      "metadata": {
        "id": "p20THoF77nbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxmdeJkI7jCP",
        "outputId": "93d5cb5e-1f8b-44f4-81fe-3b9e589a7d17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[3.7324066 4.459643  2.823731  3.4364076]\n",
            "  [3.7186127 4.439895  2.8112497 3.428871 ]\n",
            "  [3.7297044 4.4558406 2.8213637 3.4350028]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[9.7957271e-01 1.3046073e-04 2.0296868e-02]\n",
            "  [9.4339097e-01 1.6366168e-03 5.4972406e-02]\n",
            "  [9.7241926e-01 3.3523850e-04 2.7245473e-02]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
        "out2, _ = scaled_dot_product_attention(q2, k2, v2)\n",
        "\n",
        "print(\"Output from second attention head: \", out1, \"\\n\")\n",
        "print(\"Output from third attention head: \", out2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAqOWtmZ73Fh",
        "outputId": "95f86c93-f801-487f-db18-ea3a751eceb5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from second attention head:  tf.Tensor(\n",
            "[[[3.3065586 3.4400454 3.1833153 3.4144921]\n",
            "  [3.2400954 3.4136553 3.1471026 3.3772635]\n",
            "  [3.2602277 3.4184432 3.157498  3.3891182]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Output from third attention head:  tf.Tensor(\n",
            "[[[4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "  [4.1312056 4.2749963 3.316106  2.6021001]\n",
            "  [4.1585345 4.3017426 3.3267047 2.6109216]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will combine the outputs from individual head into one and then pass to linear layer"
      ],
      "metadata": {
        "id": "dYmF6Cl47-qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
        "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
        "print(combined_out_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ZH2LsR77zx",
        "outputId": "491f3b7f-eca0-4e9a-e980-e37ecd1243e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output from all heads (1, 3, 12):\n",
            "[[[3.7324066 4.459643  2.823731  3.4364076 3.3065586 3.4400454 3.1833153\n",
            "   3.4144921 4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "  [3.7186127 4.439895  2.8112497 3.428871  3.2400954 3.4136553 3.1471026\n",
            "   3.3772635 4.1312056 4.2749963 3.316106  2.6021001]\n",
            "  [3.7297044 4.4558406 2.8213637 3.4350028 3.2602277 3.4184432 3.157498\n",
            "   3.3891182 4.1585345 4.3017426 3.3267047 2.6109216]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a single matrix for all head"
      ],
      "metadata": {
        "id": "xxYqGLja8Z4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For single head this was these were the separate query matrix\n",
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOn3Wupr8Lcg",
        "outputId": "0fc1a33c-4341-43fa-a20c-6f230df056a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.8 0.3 0.9 0.2]\n",
            " [0.5 0.6 0.7 0.8]\n",
            " [0.5 0.2 0.2 0.8]\n",
            " [0.7 0.3 0.3 0.7]\n",
            " [0.8 0.9 0.7 0.6]\n",
            " [0.3 0.3 1.  0. ]\n",
            " [0.9 0.2 0.6 0.7]\n",
            " [0.1 0.  0.3 1. ]\n",
            " [1.  0.4 0.  0.1]\n",
            " [0.4 0.7 0.8 0. ]\n",
            " [0.1 0.8 0.3 0.9]\n",
            " [0.6 0.8 0.6 0.9]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[0.4 0.1 0.5 0.5]\n",
            " [0.2 0.2 0.6 0.2]\n",
            " [0.8 0.9 0.7 0.4]\n",
            " [0.3 0.9 0.3 0.4]\n",
            " [0.6 0.6 0.4 0.5]\n",
            " [0.9 0.4 0.4 0.8]\n",
            " [0.7 0.2 0.9 0.1]\n",
            " [0.6 0.7 0.8 0.8]\n",
            " [0.3 0.6 0.9 0. ]\n",
            " [0.1 0.3 0.6 1. ]\n",
            " [0.9 0.7 0.9 0.2]\n",
            " [0.3 0.9 0.3 0.4]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[1.  0.4 0.  0.8]\n",
            " [1.  0.  0.9 0.9]\n",
            " [0.6 0.7 0.1 0.8]\n",
            " [0.  0.6 0.8 0.2]\n",
            " [0.8 0.8 0.5 0.1]\n",
            " [0.1 0.1 0.2 0.5]\n",
            " [0.7 0.1 0.  0.4]\n",
            " [0.8 0.8 0.7 0.7]\n",
            " [0.7 0.7 0.3 0.3]\n",
            " [0.3 0.1 0.8 1. ]\n",
            " [0.5 0.6 0.7 0. ]\n",
            " [0.6 0.  0.5 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using a single matrix instead\n",
        "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
        "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO049gUo8trK",
        "outputId": "8a40ad3c-1d14-48bb-d639-ccd9cb6b42a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single query weight matrix (12, 12): \n",
            " [[0.8 0.3 0.9 0.2 0.4 0.1 0.5 0.5 1.  0.4 0.  0.8]\n",
            " [0.5 0.6 0.7 0.8 0.2 0.2 0.6 0.2 1.  0.  0.9 0.9]\n",
            " [0.5 0.2 0.2 0.8 0.8 0.9 0.7 0.4 0.6 0.7 0.1 0.8]\n",
            " [0.7 0.3 0.3 0.7 0.3 0.9 0.3 0.4 0.  0.6 0.8 0.2]\n",
            " [0.8 0.9 0.7 0.6 0.6 0.6 0.4 0.5 0.8 0.8 0.5 0.1]\n",
            " [0.3 0.3 1.  0.  0.9 0.4 0.4 0.8 0.1 0.1 0.2 0.5]\n",
            " [0.9 0.2 0.6 0.7 0.7 0.2 0.9 0.1 0.7 0.1 0.  0.4]\n",
            " [0.1 0.  0.3 1.  0.6 0.7 0.8 0.8 0.8 0.8 0.7 0.7]\n",
            " [1.  0.4 0.  0.1 0.3 0.6 0.9 0.  0.7 0.7 0.3 0.3]\n",
            " [0.4 0.7 0.8 0.  0.1 0.3 0.6 1.  0.3 0.1 0.8 1. ]\n",
            " [0.1 0.8 0.3 0.9 0.9 0.7 0.9 0.2 0.5 0.6 0.7 0. ]\n",
            " [0.6 0.8 0.6 0.9 0.3 0.9 0.3 0.4 0.6 0.  0.5 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
        "wv = np.concatenate((wv0, wv1, wv2), axis=1)\n",
        "\n",
        "print(f\"Single key weight matrix {wk.shape}:\\n\", wk, \"\\n\")\n",
        "print(f\"Single value weight matrix {wv.shape}:\\n\", wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql-qGNJA8_hI",
        "outputId": "d73d21cd-6d10-48fa-a9a2-d7f4cb72d28c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single key weight matrix (12, 12):\n",
            " [[0.9 0.9 0.6 0.8 0.2 0.4 0.  0.8 0.6 0.9 0.7 0.6]\n",
            " [0.9 0.5 0.  0.2 0.7 0.3 0.9 0.8 0.5 0.8 0.  0.9]\n",
            " [0.2 1.  0.  0.7 0.9 0.8 0.5 0.9 0.5 0.5 0.6 0.4]\n",
            " [1.  0.3 0.1 0.5 0.9 0.2 0.9 0.1 0.3 0.  0.9 0.6]\n",
            " [0.8 0.1 0.8 0.6 0.2 0.2 0.  0.1 0.8 0.7 0.4 0.6]\n",
            " [0.1 1.  0.9 1.  0.2 0.6 0.8 0.4 0.6 0.5 0.  0.2]\n",
            " [0.3 0.3 0.9 0.3 0.3 0.1 0.2 0.2 0.5 0.3 0.3 0.7]\n",
            " [0.7 0.5 0.9 0.4 1.  0.  0.9 0.8 0.  0.  0.7 0.7]\n",
            " [0.  0.6 0.3 0.3 0.6 1.  0.8 0.  0.5 0.6 0.2 0.3]\n",
            " [0.7 0.9 0.5 0.7 0.6 0.3 0.  0.6 0.5 0.2 0.4 0.1]\n",
            " [0.8 1.  0.  0.  0.8 0.5 0.5 0.4 0.2 0.9 0.2 0.5]\n",
            " [0.8 1.  0.2 0.8 0.3 0.7 0.6 0.2 0.7 0.2 0.5 0.8]] \n",
            "\n",
            "Single value weight matrix (12, 12):\n",
            " [[0.5 1.  0.9 0.4 0.2 1.  0.2 0.4 0.9 0.8 0.4 0.4]\n",
            " [0.3 0.3 0.6 0.8 0.7 0.7 0.1 0.4 0.1 0.  0.2 0.3]\n",
            " [0.6 0.2 0.  0.7 0.2 0.  0.3 0.5 1.  0.7 0.1 0.1]\n",
            " [0.6 0.4 0.4 0.2 0.1 0.3 1.  1.  0.3 0.2 0.  0.9]\n",
            " [0.2 0.9 0.9 0.3 0.8 0.4 0.5 0.5 0.5 0.9 0.7 0.2]\n",
            " [0.8 0.5 0.4 1.  0.3 0.  0.8 0.8 0.8 0.7 0.4 0.1]\n",
            " [0.9 0.6 0.1 0.3 0.8 0.7 0.5 0.6 0.3 0.4 0.8 0.6]\n",
            " [1.  0.  0.3 0.2 0.3 0.7 0.6 0.9 0.4 0.4 0.7 0.4]\n",
            " [0.7 0.3 0.  0.8 0.3 1.  1.  0.8 0.1 0.4 0.6 0.2]\n",
            " [0.7 0.9 0.3 0.3 0.3 0.1 0.2 0.6 0.4 0.8 0.7 0.5]\n",
            " [0.4 0.8 0.2 0.8 0.6 0.8 0.7 0.2 1.  0.9 0.1 0.7]\n",
            " [0.5 1.  0.4 0.6 0.9 0.8 0.6 0.4 1.  0.6 0.9 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can perform a single operation to calculate queries, keys and values across multiple heads\n",
        "q_s = np.dot(x, wq)\n",
        "k_s = np.dot(x, wk)\n",
        "v_s = np.dot(x, wv)"
      ],
      "metadata": {
        "id": "TCMsOJxL9ER8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7HzlwLE9a5y",
        "outputId": "0644a046-d7d0-42e3-b83f-119ad6b6c41e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vectors using a single weight matrix (1, 3, 12):\n",
            " [[[3.43 3.45 3.97 3.73 3.72 3.42 3.99 3.21 4.07 2.69 2.89 3.03]\n",
            "  [2.79 2.19 2.58 2.99 2.43 2.78 3.08 2.25 2.52 1.77 2.38 2.78]\n",
            "  [3.68 3.05 3.42 2.92 2.95 2.84 3.63 2.4  3.96 2.28 2.34 2.57]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhA5fvwD9gH_",
        "outputId": "759bf901-f88c-4d24-f699-2ec2d0ed8070"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[3.43 3.45 3.97 3.73]\n",
            "  [2.79 2.19 2.58 2.99]\n",
            "  [3.68 3.05 3.42 2.92]]] \n",
            "\n",
            "[[[3.72 3.42 3.99 3.21]\n",
            "  [2.43 2.78 3.08 2.25]\n",
            "  [2.95 2.84 3.63 2.4 ]]] \n",
            "\n",
            "[[[4.07 2.69 2.89 3.03]\n",
            "  [2.52 1.77 2.38 2.78]\n",
            "  [3.96 2.28 2.34 2.57]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have the concatenated vector, we need to separate the heads in some way so that separate self-attention operation can be performed"
      ],
      "metadata": {
        "id": "TbzwsOCH-QsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to *reshape* our combined queries from a shape of:<br>\n",
        "(batch_size, seq_len, embed_dim)<br>\n",
        "\n",
        "into a shape of<br>\n",
        " (batch_size, seq_len, num_heads, head_dim).\n",
        " <br>"
      ],
      "metadata": {
        "id": "7GJVOCvH-gYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_reshaped = tf.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))\n",
        "print(f\"Combined queries: {q_s.shape}\\n\", q_s, \"\\n\")\n",
        "print(f\"Reshaped into separate heads: {q_s_reshaped.shape}\\n\", q_s_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzoGyTr894X0",
        "outputId": "a7dd798e-ede5-4f58-ad57-a6605e21007c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined queries: (1, 3, 12)\n",
            " [[[3.43 3.45 3.97 3.73 3.72 3.42 3.99 3.21 4.07 2.69 2.89 3.03]\n",
            "  [2.79 2.19 2.58 2.99 2.43 2.78 3.08 2.25 2.52 1.77 2.38 2.78]\n",
            "  [3.68 3.05 3.42 2.92 2.95 2.84 3.63 2.4  3.96 2.28 2.34 2.57]]] \n",
            "\n",
            "Reshaped into separate heads: (1, 3, 3, 4)\n",
            " tf.Tensor(\n",
            "[[[[3.43 3.45 3.97 3.73]\n",
            "   [3.72 3.42 3.99 3.21]\n",
            "   [4.07 2.69 2.89 3.03]]\n",
            "\n",
            "  [[2.79 2.19 2.58 2.99]\n",
            "   [2.43 2.78 3.08 2.25]\n",
            "   [2.52 1.77 2.38 2.78]]\n",
            "\n",
            "  [[3.68 3.05 3.42 2.92]\n",
            "   [2.95 2.84 3.63 2.4 ]\n",
            "   [3.96 2.28 2.34 2.57]]]], shape=(1, 3, 3, 4), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By transposing, our matrix dimensions become:\n",
        "(batch_size, num_heads, seq_len, head_dim)"
      ],
      "metadata": {
        "id": "s8Riw_uUABTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_transposed = tf.transpose(q_s_reshaped, perm=[0, 2, 1, 3]).numpy()\n",
        "print(f\"Queries transposed into \\\"separate\\\" heads {q_s_transposed.shape}:\\n\",\n",
        "      q_s_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfOOHK91-2-I",
        "outputId": "9a20eace-dfa6-4466-ff81-fb307ea6e07e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries transposed into \"separate\" heads (1, 3, 3, 4):\n",
            " [[[[3.43 3.45 3.97 3.73]\n",
            "   [2.79 2.19 2.58 2.99]\n",
            "   [3.68 3.05 3.42 2.92]]\n",
            "\n",
            "  [[3.72 3.42 3.99 3.21]\n",
            "   [2.43 2.78 3.08 2.25]\n",
            "   [2.95 2.84 3.63 2.4 ]]\n",
            "\n",
            "  [[4.07 2.69 2.89 3.03]\n",
            "   [2.52 1.77 2.38 2.78]\n",
            "   [3.96 2.28 2.34 2.57]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The separate per-head query matrices from before: \")\n",
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqBzAgl4_2j_",
        "outputId": "a8df0565-5dc6-4493-85c4-361e4063437d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The separate per-head query matrices from before: \n",
            "[[[3.43 3.45 3.97 3.73]\n",
            "  [2.79 2.19 2.58 2.99]\n",
            "  [3.68 3.05 3.42 2.92]]] \n",
            "\n",
            "[[[3.72 3.42 3.99 3.21]\n",
            "  [2.43 2.78 3.08 2.25]\n",
            "  [2.95 2.84 3.63 2.4 ]]] \n",
            "\n",
            "[[[4.07 2.69 2.89 3.03]\n",
            "  [2.52 1.77 2.38 2.78]\n",
            "  [3.96 2.28 2.34 2.57]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_s_transposed = tf.transpose(tf.reshape(k_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "v_s_transposed = tf.transpose(tf.reshape(v_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "\n",
        "print(f\"Keys for all heads in a single matrix {k_s.shape}: \\n\", k_s_transposed, \"\\n\")\n",
        "print(f\"Values for all heads in a single matrix {v_s.shape}: \\n\", v_s_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm41Qg0tAIq3",
        "outputId": "b5396f60-412b-4bb3-ef19-582102bcf38e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys for all heads in a single matrix (1, 3, 12): \n",
            " [[[[4.18 4.63 3.26 3.6 ]\n",
            "   [3.11 3.43 1.69 2.58]\n",
            "   [3.42 3.97 2.96 3.15]]\n",
            "\n",
            "  [[3.27 2.59 2.55 3.02]\n",
            "   [3.18 1.92 2.24 2.42]\n",
            "   [2.6  2.7  2.51 2.27]]\n",
            "\n",
            "  [[3.31 3.43 2.59 3.47]\n",
            "   [2.29 1.91 2.34 2.56]\n",
            "   [3.16 3.23 2.1  3.18]]]] \n",
            "\n",
            "Values for all heads in a single matrix (1, 3, 12): \n",
            " [[[[3.74 4.47 2.83 3.44]\n",
            "   [3.1  2.87 1.45 2.35]\n",
            "   [3.37 3.97 2.53 3.27]]\n",
            "\n",
            "  [[3.39 3.48 3.23 3.46]\n",
            "   [2.14 2.04 2.38 2.93]\n",
            "   [3.11 3.83 3.16 3.22]]\n",
            "\n",
            "  [[4.34 4.46 3.34 2.69]\n",
            "   [2.83 2.73 2.01 2.47]\n",
            "   [3.4  3.65 3.3  2.27]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed,\n",
        "                                                                  k_s_transposed,\n",
        "                                                                  v_s_transposed)\n",
        "print(\"Self attention output:\\n\", all_heads_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymSL9PTAXF7",
        "outputId": "14da2f23-79b4-4b96-a572-0eb44804ad8a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self attention output:\n",
            " tf.Tensor(\n",
            "[[[[3.7324066 4.4596424 2.823731  3.4364076]\n",
            "   [3.7186124 4.4398947 2.8112495 3.4288704]\n",
            "   [3.7297041 4.45584   2.8213634 3.4350026]]\n",
            "\n",
            "  [[3.3065588 3.4400456 3.1833153 3.4144924]\n",
            "   [3.2400954 3.4136553 3.1471026 3.3772635]\n",
            "   [3.2602274 3.418443  3.1574974 3.389118 ]]\n",
            "\n",
            "  [[4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "   [4.131205  4.274996  3.316106  2.6021   ]\n",
            "   [4.1585345 4.3017426 3.3267047 2.6109216]]]], shape=(1, 3, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Per head outputs from using separate sets of weights per head:\")\n",
        "print(out0, \"\\n\")\n",
        "print(out1, \"\\n\")\n",
        "print(out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB2Cn0rYBB2C",
        "outputId": "03585d40-09e2-4fef-f88c-6a6c12424ad1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per head outputs from using separate sets of weights per head:\n",
            "tf.Tensor(\n",
            "[[[3.7324066 4.459643  2.823731  3.4364076]\n",
            "  [3.7186127 4.439895  2.8112497 3.428871 ]\n",
            "  [3.7297044 4.4558406 2.8213637 3.4350028]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[3.3065586 3.4400454 3.1833153 3.4144921]\n",
            "  [3.2400954 3.4136553 3.1471026 3.3772635]\n",
            "  [3.2602277 3.4184432 3.157498  3.3891182]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "  [4.1312056 4.2749963 3.316106  2.6021001]\n",
            "  [4.1585345 4.3017426 3.3267047 2.6109216]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the final concatenated result, we need to reverse our **reshape** and **transpose** operation, starting with the **transpose** this time."
      ],
      "metadata": {
        "id": "QSMlt7ITB7-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_b = tf.reshape(tf.transpose(all_heads_output, perm=[0, 2, 1, 3]),\n",
        "                            shape=(batch_size, seq_len, embed_dim))\n",
        "print(\"Final output from using single query, key, value matrices:\\n\",\n",
        "      combined_out_b, \"\\n\")\n",
        "print(\"Final output from using separate query, key, value matrices per head:\\n\",\n",
        "      combined_out_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_vr9wqxBeFV",
        "outputId": "384333f8-a825-44f5-dc53-61fb0f1f781a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output from using single query, key, value matrices:\n",
            " tf.Tensor(\n",
            "[[[3.7324066 4.4596424 2.823731  3.4364076 3.3065588 3.4400456 3.1833153\n",
            "   3.4144924 4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "  [3.7186124 4.4398947 2.8112495 3.4288704 3.2400954 3.4136553 3.1471026\n",
            "   3.3772635 4.131205  4.274996  3.316106  2.6021   ]\n",
            "  [3.7297041 4.45584   2.8213634 3.4350026 3.2602274 3.418443  3.1574974\n",
            "   3.389118  4.1585345 4.3017426 3.3267047 2.6109216]]], shape=(1, 3, 12), dtype=float32) \n",
            "\n",
            "Final output from using separate query, key, value matrices per head:\n",
            " [[[3.7324066 4.459643  2.823731  3.4364076 3.3065586 3.4400454 3.1833153\n",
            "   3.4144921 4.1941414 4.333278  3.3307385 2.625926 ]\n",
            "  [3.7186127 4.439895  2.8112497 3.428871  3.2400954 3.4136553 3.1471026\n",
            "   3.3772635 4.1312056 4.2749963 3.316106  2.6021001]\n",
            "  [3.7297044 4.4558406 2.8213637 3.4350028 3.2602277 3.4184432 3.157498\n",
            "   3.3891182 4.1585345 4.3017426 3.3267047 2.6109216]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encapsulating everything in a class"
      ],
      "metadata": {
        "id": "m85ccZtSCjOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    # Linear layer to generate final output\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights"
      ],
      "metadata": {
        "id": "h9aCNyl5CTRk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention(12, 3)\n",
        "\n",
        "output, attn_weights = mhsa(x, x, x, None)\n",
        "print(f\"MHSA output{output.shape}:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WKkMxJlkEqQ7",
        "outputId": "cf1cda59-8c05-4cc9-a50c-7599746fd12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHSA output(1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.5003724  -0.11505236  0.5572443   0.6003801   0.0584025\n",
            "    0.6416788  -0.24842958  0.3024751   1.2644374  -0.44027513\n",
            "   -0.09465392  0.25612828]\n",
            "  [-0.5384439  -0.10926842  0.56433237  0.61952585  0.01193128\n",
            "    0.67714345 -0.25823063  0.26884145  1.2858046  -0.44244006\n",
            "   -0.12061815  0.27682045]\n",
            "  [-0.44456115 -0.09979047  0.5613558   0.60058564  0.0818125\n",
            "    0.64586514 -0.23325025  0.28775805  1.2517008  -0.45000398\n",
            "   -0.05773485  0.27416545]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Encoder block"
      ],
      "metadata": {
        "id": "r6d6rycz0mlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build our **Encoder Block**. In addition to the **Multi-Head Self Attention** layer, the **Encoder Block** also has **skip connections**, **layer normalization steps**, and a **two-layer feed-forward neural network**. The original **Attention Is All You Need** paper also included some **dropout** applied to the self-attention output which isn't shown in the illustration below\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1D8sLDyQMqqhCjHWOn-I7rZKHugWxFyLy\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "U1kjNK8w0u6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "e1ePleNe0umj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
        "    msha_output = self.dropout1(mhsa_output, training=training)\n",
        "    msha_output = self.layernorm1(x + mhsa_output)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(mhsa_output + ffn_output)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "qa9BjuQvEqlG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have an embedding dimension of 12, and we want 3 attention heads and a feed forward network with a hidden dimension of 48 (4x the embedding dimension). We would declare and use a single encoder block like so:"
      ],
      "metadata": {
        "id": "JJSqQN7-BUuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_block = EncoderBlock(12, 3, 48)\n",
        "\n",
        "block_output,  _ = encoder_block(x, True, None)\n",
        "print(f\"Output from single encoder block {block_output.shape}:\")\n",
        "print(block_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRh51oBt8Idm",
        "outputId": "84cc1a4b-9e83-4106-c11e-73361974c5c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from single encoder block (1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[ 0.39541626  0.43086132 -1.9710463   0.41380224 -1.7375672\n",
            "    1.7203096   0.88450885  0.18918006 -0.16157828 -0.74061376\n",
            "    0.21856396  0.35816276]\n",
            "  [ 0.32906473  1.195967   -1.8941271   0.31242618 -1.6850877\n",
            "    1.493637    0.79473627  0.10767619 -0.19744128 -0.9286014\n",
            "    0.19759953  0.2741505 ]\n",
            "  [ 0.31259722  1.1597978  -1.9302864   0.31455603 -1.7048259\n",
            "    1.5573195   0.7670412   0.10800369 -0.22547927 -0.7777343\n",
            "    0.14544605  0.27356404]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word and positional embeddings"
      ],
      "metadata": {
        "id": "Pwr6dPp9GyLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will be dealing with the input to the encoder block, the inputs will be positional word embeddings. <br>\n",
        "We will start will a subword tokenizer called Byte-Pair Encoding"
      ],
      "metadata": {
        "id": "VJWtacP9G8Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nWFisJEBuAL",
        "outputId": "e3f78416-acce-4de8-e09b-57cf7650f816"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 577309.46B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:01<00:00, 2113733.61B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
        "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
        "print(\"Embedding size:\", bpemb_embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Qx_M16G5f6",
        "outputId": "72268741-85d8-466e-dc9a-ef6bb5b5f1d3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Embedding size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_en.vectors[bpemb_en.words.index('car')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3W-6_xJHS7P",
        "outputId": "ada08138-2bb1-4177-b325-96a485d718ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
              "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
              "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
              "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
              "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
              "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
              "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
              "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
              "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
              "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
              "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
              "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
              "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
              "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
              "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
              "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
              "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although BPEmb provides embeddings we are not interested in it since we will be having our own embedding, we just need the tokenizer from BPEmb"
      ],
      "metadata": {
        "id": "RwoFBFRNSjJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPEmb places underscores in front of any tokens which are whole words or intended to begin words."
      ],
      "metadata": {
        "id": "jRxYP5TYS1zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Where can I find a pizzeria?\"\n",
        "tokens = bpemb_en.encode(sample_sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIF54X6LHnjC",
        "outputId": "4b05b440-9f18-487f-c886-c9f0daf92de9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a pizzeria?\"))\n",
        "print(token_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62QZmQDfSdqa",
        "outputId": "55a7e035-2295-4385-a2ff-262873ffcd2c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 571  280  386 1934    4   24  248 4339  177 9967]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way to tokenize and vectorize sentences, we can declare and use an embedding layer with the same vocabulary size as **BPEmb** and a desired embedding size."
      ],
      "metadata": {
        "id": "wpGGPPs8TYjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
        "token_embeddings = token_embed(token_seq)"
      ],
      "metadata": {
        "id": "UdqoN6wkS9PW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to add *positional* information to each token embedding. As we covered in the slides, the original paper used sinusoidals but it's more common these days to just use another set of embeddings. We'll do the latter here.<br>\n",
        "\n",
        "Here, we're declaring an embedding layer with rows equalling a maximum sequence length and columns equalling our token embedding size. We then generate a vector of position ids."
      ],
      "metadata": {
        "id": "yAje3CwtTyeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "pos_idx = tf.range(len(token_seq))\n",
        "print(pos_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2u6Ro0kTy0o",
        "outputId": "a03be976-6825-41a5-d46d-4dc7c728e721"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_embeddings = pos_embed(pos_idx)\n",
        "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN364cT1UPM2",
        "outputId": "41ea0758-8131-4c02-a88f-c2571fdb1f9d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position embeddings for the input sequence\n",
            " tf.Tensor(\n",
            "[[ 1.82219483e-02 -1.69747695e-02  4.09597270e-02 -4.34763320e-02\n",
            "   4.14819755e-02  4.56087925e-02 -2.87097339e-02 -1.60238370e-02\n",
            "  -4.13637049e-02  3.21919434e-02 -1.00786611e-03  4.47556116e-02]\n",
            " [-2.40402818e-02 -1.27885714e-02  2.11089589e-02  4.15032171e-02\n",
            "   1.66279562e-02 -3.20802554e-02  1.69699453e-02  4.52707745e-02\n",
            "  -3.91988270e-02  3.49709131e-02 -4.06403057e-02  3.55405733e-03]\n",
            " [-2.60837562e-02  1.10583380e-03 -1.49048194e-02 -9.47207212e-03\n",
            "   4.72656749e-02 -3.03499401e-05 -4.26038876e-02  4.37751301e-02\n",
            "  -3.36046964e-02 -2.46626381e-02  2.73570679e-02 -2.06011776e-02]\n",
            " [-4.21421416e-02 -4.58909161e-02 -4.85099666e-02 -3.23428512e-02\n",
            "  -2.08168030e-02 -3.84754315e-02  3.21880318e-02 -3.21805105e-02\n",
            "  -1.01907961e-02 -4.71827276e-02 -2.22054962e-02  4.05672528e-02]\n",
            " [-4.70605604e-02  1.55666731e-02 -3.24656256e-02  8.93970579e-03\n",
            "  -1.66302100e-02 -2.18254924e-02 -4.13849726e-02 -1.79235339e-02\n",
            "   4.77838404e-02 -2.37017274e-02  9.97656584e-03  2.42199786e-02]\n",
            " [-3.49448435e-02  2.65434645e-02  1.89748071e-02  1.24028325e-02\n",
            "   4.22339551e-02 -3.29955705e-02  4.20548804e-02 -3.42368037e-02\n",
            "   6.06857240e-04  4.47922461e-02 -7.78733566e-03 -4.69706319e-02]\n",
            " [-5.94864041e-03 -1.21040717e-02 -6.68703392e-03 -2.76413094e-02\n",
            "   7.73646683e-03  1.23096481e-02  3.27748097e-02  3.72057222e-02\n",
            "  -4.24940251e-02  2.94234194e-02 -1.51102915e-02  3.56436409e-02]\n",
            " [ 9.53391939e-03 -2.25045923e-02 -8.56874138e-03 -2.19084620e-02\n",
            "  -1.17506161e-02 -4.52790856e-02  2.08745711e-02 -1.01792812e-03\n",
            "   4.22830619e-02  3.04408111e-02 -4.55199368e-02 -4.68559191e-03]\n",
            " [ 1.07482187e-02 -2.53483653e-02  1.63928010e-02  7.51115382e-04\n",
            "  -1.79551020e-02 -4.92745303e-02 -3.22385803e-02 -4.61358950e-03\n",
            "  -3.95565853e-02 -4.54490297e-02 -4.90709804e-02  4.23200466e-02]\n",
            " [ 3.26563381e-02  2.35574320e-03 -9.33972746e-03 -4.04952392e-02\n",
            "   8.84257630e-03 -3.22141536e-02  3.49144600e-02 -3.30031738e-02\n",
            "  -2.46648863e-03  4.65623401e-02 -4.15650755e-03  3.92156355e-02]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = token_embeddings + position_embeddings\n",
        "print(\"Input to the initial encoder block:\\n\", input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKHxUJFHUqtP",
        "outputId": "4d764b47-83a7-4d0b-dc1c-a58cc0749ded"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the initial encoder block:\n",
            " tf.Tensor(\n",
            "[[-0.0302702   0.01715881  0.05518873 -0.01663244 -0.00744355  0.01922347\n",
            "  -0.07159269  0.02735343 -0.00614367  0.05000506  0.00195571  0.01436017]\n",
            " [ 0.01254373 -0.04264393  0.03776     0.00156671 -0.02343643  0.01124756\n",
            "   0.01557662  0.05048685 -0.00756609  0.04741701  0.00406291 -0.04500313]\n",
            " [-0.00614567  0.00240604 -0.00406704  0.03552122 -0.0004468  -0.02634587\n",
            "  -0.07185796  0.03045517 -0.02015697 -0.03462595 -0.00482651 -0.03510531]\n",
            " [-0.02954583 -0.08577025 -0.07290675  0.01363734  0.0220466  -0.07063078\n",
            "   0.07918534  0.01435894 -0.04572603 -0.0679063  -0.02251847  0.00950702]\n",
            " [-0.09181182  0.0006953   0.00866111 -0.00483091 -0.02001427 -0.03301191\n",
            "  -0.02965256 -0.02992774  0.02700661 -0.04919893  0.05357183  0.01765364]\n",
            " [ 0.00092864  0.02677102  0.04778886 -0.00727808  0.05796351 -0.05613257\n",
            "   0.08918362 -0.03874144  0.01840472  0.06211065 -0.03454055 -0.00422086]\n",
            " [ 0.02542398  0.03531877 -0.02805072 -0.06887159 -0.03211389  0.03021574\n",
            "   0.04368165  0.01230893 -0.07071614 -0.00910937 -0.0167341   0.06246407]\n",
            " [-0.03542042  0.00240053 -0.03361022 -0.06238912 -0.04009397 -0.01914457\n",
            "   0.05647875 -0.01589744  0.02933828  0.06532974 -0.00924412 -0.00268177]\n",
            " [-0.03917149 -0.00406953  0.04450534 -0.02970133 -0.01012294 -0.03091774\n",
            "  -0.00861747 -0.01177505 -0.039035   -0.08465311 -0.02378257  0.03792765]\n",
            " [ 0.06603694  0.021006   -0.03884081 -0.08412667  0.03933441 -0.05544849\n",
            "   0.00805314 -0.03445695 -0.02481882  0.01915244  0.045181    0.04942222]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "1MKo6_PCV2wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim,\n",
        "               src_vocab_size, max_seq_len, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate)\n",
        "    for _ in range (num_blocks)]\n",
        "\n",
        "  def call(self, input, training, mask):\n",
        "    token_embeds = self.token_embed(input)\n",
        "\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=True)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "wCQ0RmhIU2Oz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how positional encoding is working"
      ],
      "metadata": {
        "id": "tFvG8eTbbKBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 sequences, each of length 10 (10 is also the\n",
        "# maximum sequence length in this case).\n",
        "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
        "print(seqs.shape)\n",
        "print(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEFj2XB8bDDN",
        "outputId": "174e5a90-e9f6-4a62-842f-af6432a0fd9c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[3600 5099 1095  178 4667 7867 2778 8834 9393 9274]\n",
            " [1847 4139 3060 5670 7052 6481 8522 4947 2123 8864]\n",
            " [3606 2478 6616 4787 9641 5450 2893 2018 9268 9878]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyixPz_HbM3Y",
        "outputId": "23626278-df61-4717-807d-8a4df2c34824"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS7Sb6vwb95B",
        "outputId": "6833a329-3ae0-4d38-ae80-ef26210504a5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul_Q0lKtb-FL",
        "outputId": "cf16d850-a788-43da-edca-d49bc95a42bd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[ 1.82219483e-02, -1.69747695e-02,  4.09597270e-02,\n",
              "         -4.34763320e-02,  4.14819755e-02,  4.56087925e-02,\n",
              "         -2.87097339e-02, -1.60238370e-02, -4.13637049e-02,\n",
              "          3.21919434e-02, -1.00786611e-03,  4.47556116e-02],\n",
              "        [-2.40402818e-02, -1.27885714e-02,  2.11089589e-02,\n",
              "          4.15032171e-02,  1.66279562e-02, -3.20802554e-02,\n",
              "          1.69699453e-02,  4.52707745e-02, -3.91988270e-02,\n",
              "          3.49709131e-02, -4.06403057e-02,  3.55405733e-03],\n",
              "        [-2.60837562e-02,  1.10583380e-03, -1.49048194e-02,\n",
              "         -9.47207212e-03,  4.72656749e-02, -3.03499401e-05,\n",
              "         -4.26038876e-02,  4.37751301e-02, -3.36046964e-02,\n",
              "         -2.46626381e-02,  2.73570679e-02, -2.06011776e-02],\n",
              "        [-4.21421416e-02, -4.58909161e-02, -4.85099666e-02,\n",
              "         -3.23428512e-02, -2.08168030e-02, -3.84754315e-02,\n",
              "          3.21880318e-02, -3.21805105e-02, -1.01907961e-02,\n",
              "         -4.71827276e-02, -2.22054962e-02,  4.05672528e-02],\n",
              "        [-4.70605604e-02,  1.55666731e-02, -3.24656256e-02,\n",
              "          8.93970579e-03, -1.66302100e-02, -2.18254924e-02,\n",
              "         -4.13849726e-02, -1.79235339e-02,  4.77838404e-02,\n",
              "         -2.37017274e-02,  9.97656584e-03,  2.42199786e-02],\n",
              "        [-3.49448435e-02,  2.65434645e-02,  1.89748071e-02,\n",
              "          1.24028325e-02,  4.22339551e-02, -3.29955705e-02,\n",
              "          4.20548804e-02, -3.42368037e-02,  6.06857240e-04,\n",
              "          4.47922461e-02, -7.78733566e-03, -4.69706319e-02],\n",
              "        [-5.94864041e-03, -1.21040717e-02, -6.68703392e-03,\n",
              "         -2.76413094e-02,  7.73646683e-03,  1.23096481e-02,\n",
              "          3.27748097e-02,  3.72057222e-02, -4.24940251e-02,\n",
              "          2.94234194e-02, -1.51102915e-02,  3.56436409e-02],\n",
              "        [ 9.53391939e-03, -2.25045923e-02, -8.56874138e-03,\n",
              "         -2.19084620e-02, -1.17506161e-02, -4.52790856e-02,\n",
              "          2.08745711e-02, -1.01792812e-03,  4.22830619e-02,\n",
              "          3.04408111e-02, -4.55199368e-02, -4.68559191e-03],\n",
              "        [ 1.07482187e-02, -2.53483653e-02,  1.63928010e-02,\n",
              "          7.51115382e-04, -1.79551020e-02, -4.92745303e-02,\n",
              "         -3.22385803e-02, -4.61358950e-03, -3.95565853e-02,\n",
              "         -4.54490297e-02, -4.90709804e-02,  4.23200466e-02],\n",
              "        [ 3.26563381e-02,  2.35574320e-03, -9.33972746e-03,\n",
              "         -4.04952392e-02,  8.84257630e-03, -3.22141536e-02,\n",
              "          3.49144600e-02, -3.30031738e-02, -2.46648863e-03,\n",
              "          4.65623401e-02, -4.15650755e-03,  3.92156355e-02]],\n",
              "\n",
              "       [[ 1.82219483e-02, -1.69747695e-02,  4.09597270e-02,\n",
              "         -4.34763320e-02,  4.14819755e-02,  4.56087925e-02,\n",
              "         -2.87097339e-02, -1.60238370e-02, -4.13637049e-02,\n",
              "          3.21919434e-02, -1.00786611e-03,  4.47556116e-02],\n",
              "        [-2.40402818e-02, -1.27885714e-02,  2.11089589e-02,\n",
              "          4.15032171e-02,  1.66279562e-02, -3.20802554e-02,\n",
              "          1.69699453e-02,  4.52707745e-02, -3.91988270e-02,\n",
              "          3.49709131e-02, -4.06403057e-02,  3.55405733e-03],\n",
              "        [-2.60837562e-02,  1.10583380e-03, -1.49048194e-02,\n",
              "         -9.47207212e-03,  4.72656749e-02, -3.03499401e-05,\n",
              "         -4.26038876e-02,  4.37751301e-02, -3.36046964e-02,\n",
              "         -2.46626381e-02,  2.73570679e-02, -2.06011776e-02],\n",
              "        [-4.21421416e-02, -4.58909161e-02, -4.85099666e-02,\n",
              "         -3.23428512e-02, -2.08168030e-02, -3.84754315e-02,\n",
              "          3.21880318e-02, -3.21805105e-02, -1.01907961e-02,\n",
              "         -4.71827276e-02, -2.22054962e-02,  4.05672528e-02],\n",
              "        [-4.70605604e-02,  1.55666731e-02, -3.24656256e-02,\n",
              "          8.93970579e-03, -1.66302100e-02, -2.18254924e-02,\n",
              "         -4.13849726e-02, -1.79235339e-02,  4.77838404e-02,\n",
              "         -2.37017274e-02,  9.97656584e-03,  2.42199786e-02],\n",
              "        [-3.49448435e-02,  2.65434645e-02,  1.89748071e-02,\n",
              "          1.24028325e-02,  4.22339551e-02, -3.29955705e-02,\n",
              "          4.20548804e-02, -3.42368037e-02,  6.06857240e-04,\n",
              "          4.47922461e-02, -7.78733566e-03, -4.69706319e-02],\n",
              "        [-5.94864041e-03, -1.21040717e-02, -6.68703392e-03,\n",
              "         -2.76413094e-02,  7.73646683e-03,  1.23096481e-02,\n",
              "          3.27748097e-02,  3.72057222e-02, -4.24940251e-02,\n",
              "          2.94234194e-02, -1.51102915e-02,  3.56436409e-02],\n",
              "        [ 9.53391939e-03, -2.25045923e-02, -8.56874138e-03,\n",
              "         -2.19084620e-02, -1.17506161e-02, -4.52790856e-02,\n",
              "          2.08745711e-02, -1.01792812e-03,  4.22830619e-02,\n",
              "          3.04408111e-02, -4.55199368e-02, -4.68559191e-03],\n",
              "        [ 1.07482187e-02, -2.53483653e-02,  1.63928010e-02,\n",
              "          7.51115382e-04, -1.79551020e-02, -4.92745303e-02,\n",
              "         -3.22385803e-02, -4.61358950e-03, -3.95565853e-02,\n",
              "         -4.54490297e-02, -4.90709804e-02,  4.23200466e-02],\n",
              "        [ 3.26563381e-02,  2.35574320e-03, -9.33972746e-03,\n",
              "         -4.04952392e-02,  8.84257630e-03, -3.22141536e-02,\n",
              "          3.49144600e-02, -3.30031738e-02, -2.46648863e-03,\n",
              "          4.65623401e-02, -4.15650755e-03,  3.92156355e-02]],\n",
              "\n",
              "       [[ 1.82219483e-02, -1.69747695e-02,  4.09597270e-02,\n",
              "         -4.34763320e-02,  4.14819755e-02,  4.56087925e-02,\n",
              "         -2.87097339e-02, -1.60238370e-02, -4.13637049e-02,\n",
              "          3.21919434e-02, -1.00786611e-03,  4.47556116e-02],\n",
              "        [-2.40402818e-02, -1.27885714e-02,  2.11089589e-02,\n",
              "          4.15032171e-02,  1.66279562e-02, -3.20802554e-02,\n",
              "          1.69699453e-02,  4.52707745e-02, -3.91988270e-02,\n",
              "          3.49709131e-02, -4.06403057e-02,  3.55405733e-03],\n",
              "        [-2.60837562e-02,  1.10583380e-03, -1.49048194e-02,\n",
              "         -9.47207212e-03,  4.72656749e-02, -3.03499401e-05,\n",
              "         -4.26038876e-02,  4.37751301e-02, -3.36046964e-02,\n",
              "         -2.46626381e-02,  2.73570679e-02, -2.06011776e-02],\n",
              "        [-4.21421416e-02, -4.58909161e-02, -4.85099666e-02,\n",
              "         -3.23428512e-02, -2.08168030e-02, -3.84754315e-02,\n",
              "          3.21880318e-02, -3.21805105e-02, -1.01907961e-02,\n",
              "         -4.71827276e-02, -2.22054962e-02,  4.05672528e-02],\n",
              "        [-4.70605604e-02,  1.55666731e-02, -3.24656256e-02,\n",
              "          8.93970579e-03, -1.66302100e-02, -2.18254924e-02,\n",
              "         -4.13849726e-02, -1.79235339e-02,  4.77838404e-02,\n",
              "         -2.37017274e-02,  9.97656584e-03,  2.42199786e-02],\n",
              "        [-3.49448435e-02,  2.65434645e-02,  1.89748071e-02,\n",
              "          1.24028325e-02,  4.22339551e-02, -3.29955705e-02,\n",
              "          4.20548804e-02, -3.42368037e-02,  6.06857240e-04,\n",
              "          4.47922461e-02, -7.78733566e-03, -4.69706319e-02],\n",
              "        [-5.94864041e-03, -1.21040717e-02, -6.68703392e-03,\n",
              "         -2.76413094e-02,  7.73646683e-03,  1.23096481e-02,\n",
              "          3.27748097e-02,  3.72057222e-02, -4.24940251e-02,\n",
              "          2.94234194e-02, -1.51102915e-02,  3.56436409e-02],\n",
              "        [ 9.53391939e-03, -2.25045923e-02, -8.56874138e-03,\n",
              "         -2.19084620e-02, -1.17506161e-02, -4.52790856e-02,\n",
              "          2.08745711e-02, -1.01792812e-03,  4.22830619e-02,\n",
              "          3.04408111e-02, -4.55199368e-02, -4.68559191e-03],\n",
              "        [ 1.07482187e-02, -2.53483653e-02,  1.63928010e-02,\n",
              "          7.51115382e-04, -1.79551020e-02, -4.92745303e-02,\n",
              "         -3.22385803e-02, -4.61358950e-03, -3.95565853e-02,\n",
              "         -4.54490297e-02, -4.90709804e-02,  4.23200466e-02],\n",
              "        [ 3.26563381e-02,  2.35574320e-03, -9.33972746e-03,\n",
              "         -4.04952392e-02,  8.84257630e-03, -3.22141536e-02,\n",
              "          3.49144600e-02, -3.30031738e-02, -2.46648863e-03,\n",
              "          4.65623401e-02, -4.15650755e-03,  3.92156355e-02]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try our encoder on a batch of sentences."
      ],
      "metadata": {
        "id": "AsbELQuo8RgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = [\n",
        "    \"Where can I find a pizzeria?\",\n",
        "    \"Mass hysteria over listeria.\",\n",
        "    \"I ain't no circle back girl.\"\n",
        "]\n",
        "\n",
        "bpemb_en.encode(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcJi7EJK8Jmw",
        "outputId": "b2bc5d9b-6b5d-4e30-9046-fdfc317119c2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
              " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
              " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = bpemb_en.encode_ids(input_batch)\n",
        "print(\"Vectorized inputs:\")\n",
        "input_seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cm9VQRc8OV8",
        "outputId": "6cc8aa62-2dce-42ca-c81b-af732969391b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized inputs:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
              " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
              " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input sequences aren't the same length in this batch. In this case, we need to pad them out so that they are"
      ],
      "metadata": {
        "id": "G6_wK0RT84R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
        "print(\"Input to the encoder:\")\n",
        "print(padded_input_seqs.shape)\n",
        "print(padded_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLe9CQXZ8WMp",
        "outputId": "639b3ca0-b237-40c9-90c5-a179b738e5f5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the encoder:\n",
            "(3, 10)\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our input now has padding, now's a good time to cover **masking**.\n",
        "<br>"
      ],
      "metadata": {
        "id": "zl5BqD_N9Hm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
        "print(\"Input:\")\n",
        "print(padded_input_seqs, '\\n')\n",
        "print(\"Encoder mask:\")\n",
        "print(enc_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pi9oqyH8iWD",
        "outputId": "5d9965ab-108c-4a6f-eeb7-28c9a86edf52"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]] \n",
            "\n",
            "Encoder mask:\n",
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(3, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that the dimension of the attention matrix (for this example) is going to be:<br>\n",
        "*(batch size, number of heads, query size, key size)*<br>\n",
        "(3, 3, 10, 10)"
      ],
      "metadata": {
        "id": "1jEoFEoE0c2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we need to expand the mask dimensions like so:"
      ],
      "metadata": {
        "id": "Q91YS07B0fPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "enc_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5e1pwH69U_T",
        "outputId": "12743e76-b2a9-4fe5-e4ca-3bda8ba8281b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 1, 1, 10), dtype=float32, numpy=\n",
              "array([[[[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]]],\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "       [[[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]]]],\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "       [[[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will call the encoder block like so\n",
        "\n",
        "num_encoder_blocks = 6\n",
        "\n",
        "# d_model is the embedding dimension used throughout.\n",
        "d_model = 12\n",
        "\n",
        "num_heads = 3\n",
        "\n",
        "# Feed-forward network hidden dimension width.\n",
        "ffn_hidden_dim = 48\n",
        "\n",
        "src_vocab_size = bpemb_vocab_size\n",
        "max_input_seq_len = padded_input_seqs.shape[1]\n",
        "\n",
        "encoder = Encoder(\n",
        "    num_encoder_blocks,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    ffn_hidden_dim,\n",
        "    src_vocab_size,\n",
        "    max_input_seq_len)"
      ],
      "metadata": {
        "id": "t4pfl9T89gGu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output, attn_weights = encoder(padded_input_seqs, training=True,\n",
        "                                       mask=enc_mask)\n",
        "print(f\"Encoder output {encoder_output.shape}:\")\n",
        "print(encoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNgrDJ8g2ywB",
        "outputId": "46f6484c-6900-4dd3-8629-9f8c002bf990"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output (3, 10, 12):\n",
            "tf.Tensor(\n",
            "[[[ 1.61519781e-01  6.69817746e-01 -5.12308061e-01  9.01943386e-01\n",
            "   -1.80824292e+00 -4.99264985e-01 -1.19605660e+00 -1.06476843e+00\n",
            "    5.68557143e-01  1.88536930e+00  6.48204505e-01  2.45229602e-01]\n",
            "  [ 2.68859446e-01  6.58463836e-01 -5.21348119e-01  8.90243828e-01\n",
            "   -1.81402874e+00 -5.08156538e-01 -1.20310354e+00 -1.07240808e+00\n",
            "    5.57800114e-01  1.87181807e+00  6.36754513e-01  2.35104814e-01]\n",
            "  [ 2.08389789e-01  2.37865616e-02 -4.73079890e-01  9.56770718e-01\n",
            "   -1.78249192e+00 -4.59719777e-01 -1.16366160e+00 -1.03127134e+00\n",
            "    6.20023072e-01  1.98964894e+00  7.00004756e-01  4.11600649e-01]\n",
            "  [ 1.57458305e-01  6.62578166e-01 -5.12770653e-01  8.93421054e-01\n",
            "   -1.80035233e+00 -4.99668121e-01 -1.19188237e+00 -1.06150782e+00\n",
            "    5.62080443e-01  1.90904951e+00  6.40803397e-01  2.40790635e-01]\n",
            "  [ 1.14336133e-01  6.25231743e-01 -5.63532710e-01  8.58763576e-01\n",
            "   -1.86602509e+00 -1.15231602e-02 -1.25046790e+00 -1.11877429e+00\n",
            "    5.23798883e-01  1.88618410e+00  6.03354812e-01  1.98653564e-01]\n",
            "  [ 1.57409295e-01  6.62420869e-01 -5.12791932e-01  8.93361390e-01\n",
            "   -1.80034316e+00 -4.99626964e-01 -1.19170797e+00 -1.06169224e+00\n",
            "    5.62260926e-01  1.90910542e+00  6.40787363e-01  2.40817010e-01]\n",
            "  [ 1.28938645e-01  6.57058597e-01 -5.71731567e-01  5.94867289e-01\n",
            "   -1.91822791e+00 -1.18267618e-03 -1.28193557e+00 -9.70709324e-01\n",
            "    5.52060843e-01  1.96042645e+00  6.34452105e-01  2.15983033e-01]\n",
            "  [ 1.57424152e-01  6.62432134e-01 -5.12718201e-01  8.93367171e-01\n",
            "   -1.80037165e+00 -4.99536693e-01 -1.19185698e+00 -1.06160855e+00\n",
            "    5.62138259e-01  1.90906668e+00  6.40928924e-01  2.40734711e-01]\n",
            "  [ 1.57444030e-01  6.62538767e-01 -5.12589335e-01  8.93297076e-01\n",
            "   -1.80045056e+00 -4.99510348e-01 -1.19186950e+00 -1.06161284e+00\n",
            "    5.62113106e-01  1.90902519e+00  6.40956759e-01  2.40657270e-01]\n",
            "  [ 1.14380464e-01  6.25325739e-01 -5.63582897e-01  8.58836591e-01\n",
            "   -1.86602402e+00 -1.15886936e-02 -1.25052583e+00 -1.11865318e+00\n",
            "    5.23668945e-01  1.88618565e+00  6.03303492e-01  1.98673770e-01]]\n",
            "\n",
            " [[-5.44308305e-01 -1.22658372e+00 -6.68693662e-01  5.66809177e-02\n",
            "   -1.71151781e+00 -1.98279470e-01  3.85892421e-01 -8.67488801e-01\n",
            "    1.49850523e+00  1.07717633e+00  9.46576059e-01  1.25204062e+00]\n",
            "  [-5.55134714e-01 -1.20574033e+00 -6.74973667e-01  1.57395042e-02\n",
            "   -1.66733932e+00 -2.27059379e-01  3.30069631e-01 -8.64037991e-01\n",
            "    1.38853955e+00  1.44262588e+00  8.62645924e-01  1.15466547e+00]\n",
            "  [-5.82093298e-01 -1.26504326e+00 -7.07923293e-01  1.71137433e-02\n",
            "   -1.74951673e+00  2.32779935e-01  3.47092330e-01 -9.06325519e-01\n",
            "    1.45814884e+00  1.03717268e+00  9.05985773e-01  1.21260846e+00]\n",
            "  [-5.24705172e-01 -1.22533131e+00 -6.53749287e-01  9.00758803e-02\n",
            "   -1.72242975e+00 -1.71392933e-01  4.28573042e-01 -8.57353568e-01\n",
            "    1.56844282e+00  1.13655651e+00  6.14737630e-01  1.31657636e+00]\n",
            "  [-5.52413106e-01 -1.23876977e+00 -5.97096086e-01  4.98329140e-02\n",
            "   -1.72581458e+00 -2.06168264e-01  3.81406575e-01 -8.78132522e-01\n",
            "    1.49812412e+00  1.07475841e+00  9.43144441e-01  1.25112748e+00]\n",
            "  [-5.44341981e-01 -1.22661352e+00 -6.68652117e-01  5.66770621e-02\n",
            "   -1.71151531e+00 -1.98256254e-01  3.85891289e-01 -8.67478848e-01\n",
            "    1.49852860e+00  1.07715642e+00  9.46601212e-01  1.25200331e+00]\n",
            "  [-5.14804482e-01 -1.25456345e+00 -6.40130103e-01  1.59618065e-01\n",
            "   -1.78903222e+00 -1.21521108e-01  5.23545384e-01 -8.58990669e-01\n",
            "    5.92746079e-01  1.28472006e+00  1.14012182e+00  1.47829032e+00]\n",
            "  [-5.00634074e-01 -1.54744577e+00 -5.43669879e-01  7.70039260e-02\n",
            "   -1.62604320e+00  2.84945250e-01 -2.07785331e-02 -8.13242018e-01\n",
            "    1.46605790e+00  1.06031966e+00  9.34015334e-01  1.22947180e+00]\n",
            "  [-5.42629123e-01 -1.22738075e+00 -6.68632805e-01  5.61808832e-02\n",
            "   -1.71143985e+00 -1.98616326e-01  3.86395127e-01 -8.67907226e-01\n",
            "    1.49798191e+00  1.07639110e+00  9.47077632e-01  1.25257957e+00]\n",
            "  [-4.68057841e-01 -1.20684135e+00 -6.04138851e-01  1.80188403e-01\n",
            "   -1.73100281e+00 -9.55179259e-02  7.04996660e-02 -8.18828046e-01\n",
            "    1.73904765e+00  1.28364825e+00  1.14187872e+00  5.09123981e-01]]\n",
            "\n",
            " [[-4.22553509e-01 -1.51235616e+00 -6.36033058e-01  2.61428475e-01\n",
            "   -1.53306127e+00 -3.00226390e-01  4.99154717e-01 -9.55065429e-01\n",
            "    1.44178593e+00  9.09522653e-01  1.02313960e+00  1.22426414e+00]\n",
            "  [-3.91008586e-01 -1.48419750e+00 -6.04327798e-01 -9.93772224e-02\n",
            "   -1.50360346e+00 -2.67959356e-01  5.34402311e-01 -9.25632715e-01\n",
            "    1.47782862e+00  9.44520772e-01  1.05888212e+00  1.26047289e+00]\n",
            "  [-3.91275316e-01 -1.48381674e+00 -6.04490280e-01 -9.89922136e-02\n",
            "   -1.50394332e+00 -2.67981559e-01  5.33983171e-01 -9.25364375e-01\n",
            "    1.47822750e+00  9.44388449e-01  1.05884373e+00  1.26042080e+00]\n",
            "  [-3.31029862e-01 -1.50147450e+00 -5.55205762e-01 -1.37416329e-02\n",
            "   -1.52219701e+00 -1.93569869e-01  6.69426858e-01 -9.06752825e-01\n",
            "    1.68333733e+00  1.10897887e+00  1.23372185e+00  3.28506231e-01]\n",
            "  [-3.92741919e-01 -1.48217261e+00 -6.04137897e-01 -9.94147882e-02\n",
            "   -1.50271416e+00 -2.67302752e-01  5.34185350e-01 -9.28706646e-01\n",
            "    1.47838438e+00  9.45843101e-01  1.05936480e+00  1.25941300e+00]\n",
            "  [-4.80946183e-01 -1.76721478e+00 -7.30772138e-01 -1.32927194e-01\n",
            "   -8.35368752e-01  1.47186697e-01  6.12088382e-01 -1.10839152e+00\n",
            "    1.72774565e+00  1.09520149e+00  1.23170257e+00  2.41695881e-01]\n",
            "  [-3.91449422e-01 -1.48384929e+00 -6.04293048e-01 -9.92680117e-02\n",
            "   -1.50363600e+00 -2.67674357e-01  5.34564912e-01 -9.25992548e-01\n",
            "    1.47814310e+00  9.44474936e-01  1.05867636e+00  1.26030350e+00]\n",
            "  [-3.90264928e-01 -1.48305178e+00 -6.05692804e-01 -9.82710645e-02\n",
            "   -1.50523961e+00 -2.68510997e-01  5.32235444e-01 -9.23907876e-01\n",
            "    1.47906375e+00  9.45352912e-01  1.05816019e+00  1.26012707e+00]\n",
            "  [-3.91447186e-01 -1.48391902e+00 -6.04320049e-01 -9.90996882e-02\n",
            "   -1.50376487e+00 -2.67898321e-01  5.34245968e-01 -9.25555587e-01\n",
            "    1.47813046e+00  9.44267452e-01  1.05892265e+00  1.26043797e+00]\n",
            "  [-3.90876234e-01 -1.48359466e+00 -6.04930043e-01 -9.87517089e-02\n",
            "   -1.50435901e+00 -2.68222719e-01  5.33295333e-01 -9.24812496e-01\n",
            "    1.47845387e+00  9.44766045e-01  1.05868411e+00  1.26034796e+00]]], shape=(3, 10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder block"
      ],
      "metadata": {
        "id": "o8o0inoF3GV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the **Decoder Block**. Everything we did to create the **encoder** block applies here. The major differences are that the **Decoder Block** has:\n",
        "1. a **Multi-Head Cross-Attention** layer which uses the encoder's outputs as the keys and values.\n",
        "\n",
        "2. an extra skip/residual connection along with an extra layer normalization step.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WVT4SX49bnta4uscOTF4xrsxFI4PbPER\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "VB_HhNkp3IHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.2):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output,\n",
        "                                            encoder_output,\n",
        "                                            memory_mask)\n",
        "    mhsa_output2 = self.dropout2(mhsa_output2, training=training)\n",
        "    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    output = self.layernorm3(ffn_output + mhsa_output2)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "VznMciMf23uR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "70Bv2ovM6qFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, encoder_outputs, target, training, decoder_mask, memory_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "uWKx9JxS6msL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder takes two masks:\n",
        "\n",
        "The *decoder mask* which is a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask. This mask is used in the decoder's **first** multi-head self-attention layer.\n",
        "\n",
        "The *memory mask* which is used in the decoder's **second** multi-head self-attention. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding."
      ],
      "metadata": {
        "id": "LyLxl8jS_wqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seqs = [\n",
        "    [1, 652, 723, 123, 62],\n",
        "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
        "    [1, 2369, 1259, 125, 486],\n",
        "]"
      ],
      "metadata": {
        "id": "P70DtxqG-saO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
        "print(\"Padded target inputs to the decoder:\")\n",
        "print(padded_target_input_seqs.shape)\n",
        "print(padded_target_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkRBeoUu__EQ",
        "outputId": "08b704e3-a22c-4657-f754-4d00cdb1ea02"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded target inputs to the decoder:\n",
            "(3, 8)\n",
            "[[   1  652  723  123   62    0    0    0]\n",
            " [   1   25   98  129  248  215  359  249]\n",
            " [   1 2369 1259  125  486    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
        "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(dec_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypbMM_cBAEXS",
        "outputId": "398673d5-50d4-42d1-a248-e5ef5d6e98ce"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 1, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the look-ahead mask is a diagonal where the lower half are 1s and the upper half are zeros. This is easy to create using the *band_part* method"
      ],
      "metadata": {
        "id": "cIDY1ienC-lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
        "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len,\n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "print(look_ahead_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNBCIT0EAIFh",
        "outputId": "567651ec-6b00-46d5-d747-9199c8b8cc4f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1.]], shape=(8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the decoder mask, we just need to combine the padding and look-ahead masks. Note how the columns of the resulting decoder mask are all zero for padding positions."
      ],
      "metadata": {
        "id": "iLVC-dSID0GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "print(\"The decoder mask:\")\n",
        "print(dec_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOY4bYrtDGba",
        "outputId": "b287020b-965e-42cf-cd0d-8711cb5cf48f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoder mask:\n",
            "tf.Tensor(\n",
            "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now declare a decoder and pass it everything it needs. In our case, the memory mask is the same as the encoder mask."
      ],
      "metadata": {
        "id": "RU-WbWuJEWbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(6, 12, 3, 48, 10000, 8)\n",
        "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs,\n",
        "                            True, dec_mask, enc_mask)\n",
        "print(f\"Decoder output {decoder_output.shape}:\")\n",
        "print(decoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1L_tCBzDjPR",
        "outputId": "09036582-ee32-4be9-f4e0-99deab907cac"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output (3, 8, 12):\n",
            "tf.Tensor(\n",
            "[[[-8.8539821e-01  6.0751778e-01 -1.3738725e-01 -8.6763248e-02\n",
            "    1.6306596e+00 -2.6366785e-01  1.4358404e+00 -1.4373859e+00\n",
            "   -7.8010112e-01  1.3945577e+00 -1.1337178e+00 -3.4415382e-01]\n",
            "  [-7.8027517e-01  6.3656628e-01 -6.2655085e-01  3.7751541e-01\n",
            "    1.2428114e+00 -6.2185472e-01  1.3170148e+00 -1.7722126e+00\n",
            "    2.6306489e-01  1.4296468e+00 -1.2079386e+00 -2.5778776e-01]\n",
            "  [-7.5314182e-01  7.9830962e-01 -6.1600941e-01  2.9582545e-01\n",
            "    1.5980927e+00  1.7473064e-02  1.5998267e+00 -1.6669861e+00\n",
            "   -2.9680535e-01  8.3317631e-01 -9.7582680e-01 -8.3393466e-01]\n",
            "  [-7.3061341e-01  8.1737435e-01 -7.7596700e-01  7.3493160e-03\n",
            "    1.7230958e+00  8.4926619e-04  5.7488269e-01 -1.7727554e+00\n",
            "   -5.1440674e-01  1.6677879e+00 -8.1850636e-01 -1.7909034e-01]\n",
            "  [-5.2621698e-01  6.3444841e-01 -8.7892771e-01  1.6074647e-01\n",
            "    1.2533325e+00  4.0912312e-01  1.1167920e+00 -1.9645927e+00\n",
            "   -6.2419146e-01  1.5302031e+00 -9.5763403e-01 -1.5308262e-01]\n",
            "  [-7.5922149e-01  8.6729115e-01 -6.7706788e-01  2.0625062e-01\n",
            "    1.3793687e+00 -2.1093212e-01  1.5084648e+00 -1.4876525e+00\n",
            "   -5.7874322e-01  1.2751689e+00 -1.3127085e+00 -2.1021852e-01]\n",
            "  [-7.8825831e-01  7.5326043e-01 -5.2387673e-01  1.2666127e-01\n",
            "    1.1978245e+00 -3.5241273e-01  1.3248184e+00 -1.7083676e+00\n",
            "   -5.9710133e-01  1.6290610e+00 -1.1259112e+00  6.4302251e-02]\n",
            "  [-4.1761291e-01  1.3789272e+00 -8.1444633e-01 -9.9778497e-01\n",
            "    1.9419820e+00  3.3088076e-01  9.7743893e-01 -9.5979536e-01\n",
            "    3.0449593e-01  1.4756571e-01 -1.4929948e+00 -3.9865625e-01]]\n",
            "\n",
            " [[ 3.4748983e-01 -1.5463510e-01 -9.5484793e-01  5.4182994e-01\n",
            "    1.5269680e+00 -8.6518687e-01  2.3165904e-01 -1.8051233e+00\n",
            "   -6.9853151e-01  1.8333446e+00 -4.5035484e-01  4.4738820e-01]\n",
            "  [ 5.4935640e-01 -2.0710982e-01 -1.3971788e+00  4.6455139e-01\n",
            "   -5.6872219e-01 -5.9958225e-01  5.8953297e-01 -8.3660495e-01\n",
            "   -6.5478206e-01  2.0781853e+00 -8.9937162e-01  1.4817257e+00]\n",
            "  [ 5.3285621e-02 -7.2449185e-02 -9.5648903e-01  1.0383034e+00\n",
            "    2.0674086e+00 -4.6467531e-01  1.1634175e+00 -1.0584166e+00\n",
            "   -1.1647269e+00  8.0983043e-01 -6.2565082e-01 -7.8983754e-01]\n",
            "  [ 1.4142603e-01  4.3634465e-01 -8.1099230e-01  3.8579798e-01\n",
            "    1.8437212e+00 -1.1102903e+00  1.4154992e-01 -1.7340909e+00\n",
            "   -3.5037467e-01  1.7226849e+00 -3.8756824e-01 -2.7820826e-01]\n",
            "  [ 8.2251057e-02  2.5759831e-01 -9.3443209e-01  7.1516401e-01\n",
            "    1.8296462e+00 -3.5167032e-01 -3.2529286e-01 -1.9512123e+00\n",
            "   -6.4281243e-01  1.6244237e+00 -3.1785765e-01  1.4194540e-02]\n",
            "  [ 6.3145548e-01  3.1705672e-01 -3.2287306e-01  1.0271908e+00\n",
            "    1.3868268e+00 -1.0043807e+00  5.9010732e-01 -1.3117106e+00\n",
            "   -5.4353547e-01  1.5034428e+00 -8.2606184e-01 -1.4475183e+00]\n",
            "  [ 6.1058736e-01  2.1075697e-01 -1.2771137e+00  1.0043217e+00\n",
            "    4.1417202e-01 -7.6034135e-01  5.4868573e-01 -1.6766889e+00\n",
            "   -4.4344831e-01  2.1359878e+00 -2.8102615e-01 -4.8589319e-01]\n",
            "  [ 4.1737941e-01  2.3147815e-01 -8.8651973e-01 -1.6504924e-01\n",
            "    2.1526313e+00 -8.2217693e-01  4.1242534e-01 -1.3428794e+00\n",
            "   -7.2902817e-01  1.6408516e+00 -3.3112729e-01 -5.7798505e-01]]\n",
            "\n",
            " [[-2.5358883e-01  1.0629175e+00  1.5485361e-01 -4.9641144e-01\n",
            "    3.4633914e-01 -2.8952450e-01  9.1769218e-01 -6.2353659e-01\n",
            "    3.5694346e-01  1.9885802e+00 -1.5624418e+00 -1.6018226e+00]\n",
            "  [ 5.2101362e-01  1.0003716e+00 -1.2364690e+00 -5.5950153e-01\n",
            "    4.5773467e-01 -1.9258979e-01  1.4242699e+00  2.0622596e-02\n",
            "   -1.0785481e-01  1.5506974e+00 -1.5841427e+00 -1.2941521e+00]\n",
            "  [-2.9486471e-01  4.3106738e-01  2.9334453e-01 -7.7120006e-02\n",
            "    1.7625442e+00 -1.3649625e+00  3.5485429e-01 -1.1084335e+00\n",
            "    1.4605774e-01  1.8248961e+00 -1.0773914e+00 -8.8999212e-01]\n",
            "  [ 1.3009225e-01  1.0131427e+00 -7.7981287e-01 -1.5747576e+00\n",
            "    4.0431461e-01 -6.3536751e-01  8.7782669e-01 -1.0830927e+00\n",
            "    4.2045858e-02  1.8109062e+00 -1.1137711e+00  9.0847349e-01]\n",
            "  [-1.5183967e-01  4.9500120e-01  3.4521732e-01  1.8510802e-01\n",
            "    1.1764102e+00 -1.2003660e+00  3.5213134e-01 -1.9040586e+00\n",
            "   -3.2478783e-01  2.0590556e+00 -2.7172172e-01 -7.6015019e-01]\n",
            "  [-4.3755141e-01  1.3195421e+00  4.3353984e-01 -5.1693851e-01\n",
            "    1.5616611e+00 -9.0695858e-01  1.3056055e-01 -1.0585760e+00\n",
            "   -1.1578585e-01  1.7055897e+00 -9.0373480e-01 -1.2113484e+00]\n",
            "  [ 1.1791360e-02  1.0355452e+00 -3.8267395e-01 -1.5596658e-01\n",
            "    4.8574680e-01 -1.2506645e+00  2.4943613e-01 -1.2226391e+00\n",
            "   -3.6137864e-01  2.5037994e+00 -9.9316990e-01  8.0173627e-02]\n",
            "  [-6.1801348e-02  9.5444906e-01 -1.1980528e-02  3.7789091e-01\n",
            "    1.1581496e+00 -1.2169993e+00 -1.2890555e-02 -1.7370497e+00\n",
            "   -5.3455066e-02  1.9635344e+00 -2.8476146e-01 -1.0750858e+00]]], shape=(3, 8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "8Tfa5NdeEclE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "               target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "                           max_input_len, dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_target_len, dropout_rate)\n",
        "\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
        "           decoder_mask, memory_mask):\n",
        "    encoder_output, encoder_attn_weights = self.encoder(input_seqs,\n",
        "                                                        training, encoder_mask)\n",
        "\n",
        "    decoder_output, decoder_attn_weights = self.decoder(encoder_output,\n",
        "                                                        target_input_seqs, training,\n",
        "                                                        decoder_mask, memory_mask)\n",
        "\n",
        "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights"
      ],
      "metadata": {
        "id": "XJtLLY4aEZZO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_blocks = 6,\n",
        "    d_model = 12,\n",
        "    num_heads = 3,\n",
        "    hidden_dim = 48,\n",
        "    source_vocab_size = bpemb_vocab_size,\n",
        "    target_vocab_size = 7000, # made-up target vocab size.\n",
        "    max_input_len = padded_input_seqs.shape[1],\n",
        "    max_target_len = padded_target_input_seqs.shape[1])\n",
        "\n",
        "transformer_output, _, _ = transformer(padded_input_seqs,\n",
        "                                       padded_target_input_seqs, True,\n",
        "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
        "print(f\"Transformer output {transformer_output.shape}:\")\n",
        "print(transformer_output) # If training, we would use this output to calculate losses."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_zyoAZFyTt",
        "outputId": "9d79a9e6-0237-4171-b3bd-bf1089c08316"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output (3, 8, 7000):\n",
            "tf.Tensor(\n",
            "[[[ 0.01284234  0.07227436  0.09752458 ...  0.02201883 -0.06278675\n",
            "    0.03497577]\n",
            "  [ 0.01606729  0.03789817  0.044289   ... -0.07608898 -0.03115907\n",
            "   -0.00040297]\n",
            "  [ 0.01155925  0.10143702  0.06334066 ... -0.01543371 -0.0725012\n",
            "    0.01715794]\n",
            "  ...\n",
            "  [ 0.02296371  0.0597923   0.02989318 ... -0.06274884 -0.04978283\n",
            "    0.01587684]\n",
            "  [ 0.02745717  0.10437047  0.05982758 ...  0.00036109 -0.0538918\n",
            "    0.02103401]\n",
            "  [ 0.01715882  0.08757053 -0.0079866  ... -0.09258862 -0.02295894\n",
            "    0.0021074 ]]\n",
            "\n",
            " [[-0.02423562  0.06052178  0.00236865 ... -0.03600153 -0.08614506\n",
            "   -0.02912395]\n",
            "  [ 0.01155205  0.05440085 -0.03475895 ... -0.00840835 -0.07614617\n",
            "    0.00731555]\n",
            "  [ 0.00424316  0.10854127 -0.08500981 ... -0.07976189 -0.06361163\n",
            "   -0.02422469]\n",
            "  ...\n",
            "  [ 0.02735201  0.14400566 -0.12040977 ... -0.00666932 -0.00438626\n",
            "    0.00946678]\n",
            "  [-0.00979459  0.10396613 -0.05031697 ... -0.05823579 -0.09110771\n",
            "   -0.03022914]\n",
            "  [ 0.04948877  0.11255334 -0.1052089  ... -0.03010711  0.01384205\n",
            "    0.00619302]]\n",
            "\n",
            " [[-0.0287043   0.05147304 -0.07046027 ... -0.09166002 -0.0751325\n",
            "   -0.05647764]\n",
            "  [-0.04665589  0.0125484   0.06816038 ...  0.00171668 -0.07825345\n",
            "    0.02630579]\n",
            "  [ 0.01049979  0.11847689 -0.12873623 ... -0.06757145 -0.0588332\n",
            "   -0.04996307]\n",
            "  ...\n",
            "  [ 0.00824784  0.01893299 -0.0152657  ... -0.07870663 -0.05670054\n",
            "   -0.00275596]\n",
            "  [-0.02281254  0.02191524 -0.04873084 ... -0.09651996 -0.08857472\n",
            "   -0.07667352]\n",
            "  [-0.03268742  0.08062959 -0.04648772 ... -0.07255054 -0.11589813\n",
            "   -0.05085842]]], shape=(3, 8, 7000), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpltEZqdF4i1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}